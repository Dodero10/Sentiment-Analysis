{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Input and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:27:08.224337Z",
     "iopub.status.busy": "2024-04-23T07:27:08.223639Z",
     "iopub.status.idle": "2024-04-23T07:28:08.072466Z",
     "shell.execute_reply": "2024-04-23T07:28:08.071448Z",
     "shell.execute_reply.started": "2024-04-23T07:27:08.224306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!java -version\n",
    "!pip install py_vncorenlp\n",
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:28:08.075316Z",
     "iopub.status.busy": "2024-04-23T07:28:08.074642Z",
     "iopub.status.idle": "2024-04-23T07:28:22.618651Z",
     "shell.execute_reply": "2024-04-23T07:28:22.617785Z",
     "shell.execute_reply.started": "2024-04-23T07:28:08.075283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "rdrsegmenter = VnCoreNLP('/kaggle/input/sentiment-analysis/vncorenlp/vncorenlp/VnCoreNLP-1.1.1.jar', annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:28:22.620379Z",
     "iopub.status.busy": "2024-04-23T07:28:22.619920Z",
     "iopub.status.idle": "2024-04-23T07:28:22.647893Z",
     "shell.execute_reply": "2024-04-23T07:28:22.646724Z",
     "shell.execute_reply.started": "2024-04-23T07:28:22.620351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = \"Tôi là sinh viên trường Đại học Bách Khoa Hà Nội\"\n",
    "words = rdrsegmenter.tokenize(text)\n",
    "\n",
    "encode = []\n",
    "for word in words:\n",
    "    encode.append(tokenizer.encode(word))\n",
    "\n",
    "for i in range(len(encode)):\n",
    "    print(f'Word: {words[i]}, Encode: {encode[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:28:22.649831Z",
     "iopub.status.busy": "2024-04-23T07:28:22.649434Z",
     "iopub.status.idle": "2024-04-23T07:28:22.666970Z",
     "shell.execute_reply": "2024-04-23T07:28:22.665821Z",
     "shell.execute_reply.started": "2024-04-23T07:28:22.649795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def strip_emoji(text):\n",
    "\tRE_EMOJI = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "\treturn RE_EMOJI.sub(r'', text)\n",
    "\n",
    "def remove_special_char(text):\n",
    "\tspecial_character = re.compile(\"�+\")\n",
    "\treturn special_character.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\tpunctuation = re.compile(r\"[!#$%&()*+;<=>?@[\\]^_`{|}~]\")\n",
    "\treturn punctuation.sub(r\"\", text)\n",
    "\n",
    "def remove_number(text):\n",
    "\treturn re.sub(\" \\d+\", \" \", text)\n",
    "\n",
    "def normalize_annotatation(text):\n",
    "\tkhach_san = \"\\bkhach san ?|\\bksan ?|\\bks ?\"\n",
    "\treturn re.sub(\"\\bnv ?\", \"nhân viên\",re.sub(khach_san, \"khách sạn\", text))\n",
    "\n",
    "def clean_text(text):\n",
    "\treturn {\"Review\": normalize_annotatation(remove_number(remove_special_char(remove_punctuation(strip_emoji(text[\"Review\"].lower())))))}\n",
    "\n",
    "train_data = []\n",
    "train_set_path = '/kaggle/input/vqadat/vaq2.0.TrainImages.txt'\n",
    "\n",
    "with open(train_set_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "        else:\n",
    "            answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0][:-2],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        train_data.append(data_sample)\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, tokenizer, rdrsegmenter):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rdrsegmenter = rdrsegmenter\n",
    "        self.feature = ['giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'di_chuyen', 'mua_sam']\n",
    "\n",
    "    def segment(self, example):\n",
    "        return {\"Segment\": \" \".join([\" \".join(sen) for sen in self.rdrsegmenter.tokenize(example[\"Review\"])])}\n",
    " \n",
    "    def tokenize(self, example):\n",
    "        return self.tokenizer(example[\"Segment\"], truncation=True)\n",
    "    \n",
    "    def label(self, example):\n",
    "        return {'labels_regressor': np.array([example[i] for i in self.feature]),\n",
    "            'labels_classifier': np.array([int(example[i] != 0) for i in self.feature])}\n",
    "        \n",
    "    def run(self, dataset):\n",
    "        dataset = dataset.map(clean_text)\n",
    "        dataset = dataset.map(self.segment)\n",
    "        dataset = dataset.map(self.tokenize, batched=True)\n",
    "        dataset = dataset.map(self.label)\n",
    "        dataset = dataset.remove_columns(['Unnamed: 0','Review', 'giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'di_chuyen', 'mua_sam', 'Segment'])\n",
    "        dataset.set_format(\"torch\")\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:28:22.671223Z",
     "iopub.status.busy": "2024-04-23T07:28:22.670922Z",
     "iopub.status.idle": "2024-04-23T07:28:24.646619Z",
     "shell.execute_reply": "2024-04-23T07:28:24.645820Z",
     "shell.execute_reply.started": "2024-04-23T07:28:22.671198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': '/kaggle/input/sentiment-analysis/data/data/vi/train_datasets.csv',\n",
    "    'test': '/kaggle/input/sentiment-analysis/data/data/vi/test_datasets.csv',\n",
    "}\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:28:24.648321Z",
     "iopub.status.busy": "2024-04-23T07:28:24.647864Z",
     "iopub.status.idle": "2024-04-23T07:29:07.570581Z",
     "shell.execute_reply": "2024-04-23T07:29:07.569692Z",
     "shell.execute_reply.started": "2024-04-23T07:28:24.648292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocess = Preprocess(tokenizer, rdrsegmenter)\n",
    "tokenizer_datasets = preprocess.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:29:07.572200Z",
     "iopub.status.busy": "2024-04-23T07:29:07.571883Z",
     "iopub.status.idle": "2024-04-23T07:29:17.533816Z",
     "shell.execute_reply": "2024-04-23T07:29:17.532973Z",
     "shell.execute_reply.started": "2024-04-23T07:29:07.572174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataloader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(tokenizer_datasets['train'], collate_fn=data_collator, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenizer_datasets['test'], collate_fn=data_collator, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:29:17.535512Z",
     "iopub.status.busy": "2024-04-23T07:29:17.534976Z",
     "iopub.status.idle": "2024-04-23T07:29:17.546580Z",
     "shell.execute_reply": "2024-04-23T07:29:17.545685Z",
     "shell.execute_reply.started": "2024-04-23T07:29:17.535486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModelSoftmax(nn.Module):\n",
    "\tdef __init__(self, checkpoint):\n",
    "\t\tsuper(CustomModelSoftmax, self).__init__()\n",
    "\t\tself.model = model = AutoModel.from_config(AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "\t\tself.dropout = nn.Dropout(0.1)\n",
    "\t\tself.classifier = nn.Linear(768*4, 6)\n",
    "\t\tself.regressor = nn.Linear(768*4, 30)\n",
    "  \n",
    "\tdef forward(self, input_ids=None, attention_mask=None):\n",
    "\t\toutputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\toutputs = torch.cat((outputs[2][-1][:,0, ...],outputs[2][-2][:,0, ...], outputs[2][-3][:,0, ...], outputs[2][-4][:,0, ...]),-1)\n",
    "\n",
    "\t\toutputs = self.dropout(outputs)\n",
    "  \n",
    "\t\toutputs_classifier = self.classifier(outputs)\n",
    "\t\toutputs_regressor = self.regressor(outputs)\n",
    "  \n",
    "\t\toutputs_classifier = nn.Sigmoid()(outputs_classifier)\n",
    "\t\toutputs_regressor = outputs_regressor.reshape(-1, 6, 5)\n",
    "  \n",
    "\t\treturn outputs_classifier, outputs_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:29:17.548312Z",
     "iopub.status.busy": "2024-04-23T07:29:17.547865Z",
     "iopub.status.idle": "2024-04-23T07:29:21.731512Z",
     "shell.execute_reply": "2024-04-23T07:29:21.730506Z",
     "shell.execute_reply.started": "2024-04-23T07:29:17.548277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# Model \n",
    "model = CustomModelSoftmax(\"vinai/phobert-base\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:29:21.733237Z",
     "iopub.status.busy": "2024-04-23T07:29:21.732935Z",
     "iopub.status.idle": "2024-04-23T07:29:21.749839Z",
     "shell.execute_reply": "2024-04-23T07:29:21.748851Z",
     "shell.execute_reply.started": "2024-04-23T07:29:21.733213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss_classifier(pred_classifier, labels_classifier):\n",
    "    return nn.BCELoss()(pred_classifier, labels_classifier)\n",
    "\n",
    "def loss_regressor(pred_regressor, labels_regressor):\n",
    "    mask = (labels_regressor != 0)\n",
    "    loss = ((pred_regressor - labels_regressor)**2)[mask].sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "def loss_softmax(inputs, labels, device):\n",
    "    mask = (labels != 0)\n",
    "    # inputs (N, 6, 5)\n",
    "    n, aspect, rate = inputs.shape\n",
    "    num = 0\n",
    "    loss = torch.zeros(labels.shape).to(device)\n",
    "    for i in range(aspect):\n",
    "        label_i = labels[:, i].clone()\n",
    "        label_i[label_i != 0] -= 1\n",
    "        label_i = label_i.type(torch.LongTensor).to(device)\n",
    "        loss[:, i] = nn.CrossEntropyLoss(reduction='none')(inputs[:, i, :], label_i)\n",
    "    loss = loss[mask].sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = 0.25,\n",
    "    gamma: float = 2,\n",
    "    reduction: str = \"none\",):\n",
    "\n",
    "    # p = torch.sigmoid(inputs)\n",
    "    p = inputs\n",
    "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def bce_loss_weights(inputs, targets, weights):\n",
    "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    weights = targets*(1 / weights.view(1, -1)) + (1 - targets)*(1 / (1 - weights.view(1, -1)))\n",
    "    loss = ce_loss*weights\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def CB_loss(inputs, targets, samples_positive_per_cls, samples_negative_per_cls, no_of_classes=2,loss_type='sigmoid', beta=0.9999, gamma=2):\n",
    "    samples_per_cls = torch.concat([samples_positive_per_cls.unsqueeze(-1), samples_negative_per_cls.unsqueeze(-1)], dim=-1) # num_cls, 2\n",
    "    effective_num = 1.0 - torch.pow(beta, samples_per_cls) # num_cls, 2\n",
    "    weights = (1.0 - beta) / effective_num # num_cls, 2\n",
    "    weights = weights / weights.sum(dim=-1).reshape(-1, 1) * no_of_classes # num_cls, 2 \n",
    "    weights = targets*weights[:, 0] + (1 - targets)*weights[:, 1]\n",
    "\n",
    "    if loss_type == \"focal\":\n",
    "        cb_loss = (sigmoid_focal_loss(inputs, targets)*weights).mean()\n",
    "    elif loss_type == \"sigmoid\":\n",
    "        cb_loss = (F.binary_cross_entropy(inputs,targets, reduction=\"none\")*weights).mean()\n",
    "    return cb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:29:21.751629Z",
     "iopub.status.busy": "2024-04-23T07:29:21.751241Z",
     "iopub.status.idle": "2024-04-23T07:29:21.778814Z",
     "shell.execute_reply": "2024-04-23T07:29:21.777915Z",
     "shell.execute_reply.started": "2024-04-23T07:29:21.751596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs*len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:40:47.820956Z",
     "iopub.status.busy": "2024-04-23T07:40:47.819930Z",
     "iopub.status.idle": "2024-04-23T07:40:47.840593Z",
     "shell.execute_reply": "2024-04-23T07:40:47.839527Z",
     "shell.execute_reply.started": "2024-04-23T07:40:47.820919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScalarMetric():\n",
    "    def __init__(self):\n",
    "        self.scalar = 0\n",
    "        self.num = 0\n",
    "    def update(self, scalar):\n",
    "        self.scalar += scalar\n",
    "        self.num += 1\n",
    "        return self\n",
    "    def compute(self):\n",
    "        return self.scalar / self.num\n",
    "    def reset(self):\n",
    "        self.scalar = 0\n",
    "        self.num = 0\n",
    "\n",
    "class AccuracyMetric():\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "        self.num = 0\n",
    "    def update(self, y_pred, y_true):\n",
    "        self.correct += (y_pred == y_true).sum()\n",
    "        self.num += len(y_pred)*y_pred.shape[1]\n",
    "    def compute(self):\n",
    "        return self.correct / self.num\n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.num = 0\n",
    "\n",
    "def precision(y_pred, y_true):\n",
    "    true_positive = np.logical_and(y_pred, y_true).sum(axis=0)\n",
    "    false_positive = np.logical_and(y_pred, np.logical_not(y_true)).sum(axis=0)\n",
    "    return true_positive / (true_positive + false_positive)\n",
    "\n",
    "def recall(y_pred, y_true):\n",
    "    true_positive = np.logical_and(y_pred, y_true).sum(axis=0)\n",
    "    false_negative = np.logical_and(np.logical_not(y_pred), y_true).sum(axis=0)\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "class F1_score():\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "    def update(self, y_pred, y_true):\n",
    "        self.y_pred = np.concatenate([self.y_pred, y_pred], axis=0) if self.y_pred is not None else y_pred\n",
    "        self.y_true = np.concatenate([self.y_true, y_true], axis=0) if self.y_true is not None else y_true\n",
    "    def compute(self):\n",
    "        f1_score = np.zeros(self.y_pred.shape[1])\n",
    "        precision_score = precision(self.y_pred != 0, self.y_true != 0)\n",
    "        recall_score = recall(self.y_pred != 0, self.y_true != 0)\n",
    "        mask_precision_score = np.logical_and(precision_score != 0, np.logical_not(np.isnan(precision_score)))\n",
    "        mask_recall_score = np.logical_and(recall_score != 0, np.logical_not(np.isnan(recall_score)))\n",
    "        mask = np.logical_and(mask_precision_score, mask_recall_score)\n",
    "        print(\"Precision:\",precision_score)\n",
    "        print(\"Recall\", recall_score)\n",
    "        f1_score[mask] = 2* (precision_score[mask] * recall_score[mask]) / (precision_score[mask] + recall_score[mask])\n",
    "        return f1_score\n",
    "\n",
    "class R2_score():\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, y_pred, y_true):\n",
    "        self.y_pred = np.concatenate([self.y_pred, y_pred], axis=0) if self.y_pred is not None else y_pred\n",
    "        self.y_true = np.concatenate([self.y_true, y_true], axis=0) if self.y_true is not None else y_true\n",
    "    \n",
    "    def compute(self):\n",
    "        mask = np.logical_and(self.y_pred !=0, self.y_true != 0)\n",
    "        rss = (((self.y_pred - self.y_true)**2)*mask).sum(axis=0) \n",
    "        k = (mask*16).sum(axis=0)\n",
    "        r2_score = np.ones(rss.shape[0])\n",
    "        mask2 = (k != 0)\n",
    "        r2_score[mask2] = 1 - rss[mask2]/k[mask2]\n",
    "        return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:14.066213Z",
     "iopub.status.busy": "2024-04-23T07:44:14.065273Z",
     "iopub.status.idle": "2024-04-23T07:44:14.077256Z",
     "shell.execute_reply": "2024-04-23T07:44:14.076245Z",
     "shell.execute_reply.started": "2024-04-23T07:44:14.066180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_size):\n",
    "\tshuffled = np.random.permutation(len(data))\n",
    "\tnum_test = int(test_size*len(data))\n",
    "\ttest_index = shuffled[:num_test]\n",
    "\ttrain_index = shuffled[num_test:]\n",
    "\treturn data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "def prob_to_label_1(pred):\n",
    "\tmask = (pred >= 0.5)\n",
    "\tx_coor, y_coor = np.where(mask)\n",
    "\tresult = np.zeros((pred.shape[0], 6))\n",
    "\tfor x, y in zip(x_coor, y_coor):\n",
    "\t\tloc = y // 6\n",
    "\t\tstar = y % 6\n",
    "\t\tresult[x][loc] = star\n",
    "\treturn result\n",
    "\n",
    "def prob_to_label_2(pred):\n",
    "\tresult = np.zeros((pred.shape[0], 6))\n",
    "\tpred = pred.reshape(pred.shape[0], -1, 5)\n",
    "\tstar = pred.argmax(axis=-1) + 1\n",
    "\tprob = pred.max(axis=-1)\n",
    "\tmask = prob >= 0.5\n",
    "\tresult[mask] = star[mask]\n",
    "\treturn result\n",
    "\n",
    "def pred_to_label(outputs_classifier, outputs_regressor):\n",
    "\t\"\"\"Convert output model to label. Get aspects have reliability >= 0.5\n",
    "\n",
    "\tArgs:\n",
    "\t\toutputs_classifier (numpy.array): Output classifier layer\n",
    "\t\toutputs_regressor (numpy.array): Output regressor layer\n",
    "\n",
    "\tReturns:\n",
    "\t\tpredicted label\n",
    "\t\"\"\"\n",
    "\tresult = np.zeros((outputs_classifier.shape[0], 6))\n",
    "\tmask = (outputs_classifier >= 0.5)\n",
    "\tresult[mask] = outputs_regressor[mask]\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T08:03:03.876725Z",
     "iopub.status.busy": "2024-04-23T08:03:03.876196Z",
     "iopub.status.idle": "2024-04-23T08:16:37.387864Z",
     "shell.execute_reply": "2024-04-23T08:16:37.386802Z",
     "shell.execute_reply.started": "2024-04-23T08:03:03.876669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "\n",
    "# Training\n",
    "pb_train = tqdm(range(num_training_steps))\n",
    "pb_test = tqdm(range(num_epochs*len(test_dataloader)))\n",
    "best_score = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)}\n",
    "        outputs_classifier, outputs_regressor = model(**inputs)\n",
    "        loss1 = sigmoid_focal_loss(outputs_classifier, batch['labels_classifier'].to(device).float(), alpha=-1, gamma=1,reduction='mean')\n",
    "        loss2 = loss_softmax(outputs_regressor, batch['labels_regressor'].to(device).float(), device)\n",
    "        loss = 10*loss1 + loss2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        lr_scheduler.step()\n",
    "        pb_train.update(1)\n",
    "        pb_train.set_postfix(loss_classifier=loss1.item(),loss_regressor=loss2.item(),loss=loss.item())\n",
    "        train_loss += loss.item() / len(train_dataloader)\n",
    "    print(\"Train Loss:\", train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    # model.eval()\n",
    "    val_loss = ScalarMetric()\n",
    "    val_loss_classifier = ScalarMetric()\n",
    "    val_loss_regressor = ScalarMetric()\n",
    "    val_acc = AccuracyMetric()\n",
    "    val_f1_score = F1_score()\n",
    "    val_r2_score = R2_score()\n",
    "    num = 0\n",
    "    correct = 0\n",
    "    result = None\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)}\n",
    "        with torch.no_grad():\n",
    "            outputs_classifier, outputs_regressor = model(**inputs)\n",
    "            loss1 = loss_classifier(outputs_classifier, batch['labels_classifier'].to(device).float())\n",
    "            loss2 = loss_softmax(outputs_regressor, batch['labels_regressor'].to(device).float(), device)\n",
    "            loss = loss1 + loss2\n",
    "            outputs_classifier = outputs_classifier.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.argmax(axis=-1) + 1\n",
    "            y_true = batch['labels_regressor'].numpy()\n",
    "            outputs = pred_to_label(outputs_classifier, outputs_regressor)\n",
    "            result = np.concatenate([result, np.round(outputs)], axis=0) if result is not None else np.round(outputs)\n",
    "            val_loss_classifier.update(loss1.item())\n",
    "            val_loss_regressor.update(loss2.item())\n",
    "            val_loss.update(loss.item())\n",
    "            val_acc.update(np.round(outputs), y_true)\n",
    "            val_f1_score.update(np.round(outputs), y_true)\n",
    "            val_r2_score.update(np.round(outputs), y_true)\n",
    "            pb_test.update(1)\n",
    "            \n",
    "    f1_score = val_f1_score.compute()\n",
    "    r2_score = val_r2_score.compute()\n",
    "    final_score = (f1_score * r2_score).sum()*1/6\n",
    "    \n",
    "    if final_score > best_score:\n",
    "        best_score = final_score\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/model.pt\")\n",
    "        \n",
    "    print(\"Test Loss:\", val_loss.compute(), \"Loss Classifier:\", val_loss_classifier.compute(), \"Loss Regressor:\", val_loss_regressor.compute())\n",
    "    print(\"Acc\", val_acc.compute())\n",
    "    print(\"F1_score\", f1_score)\n",
    "    print(\"R2_score\", r2_score)\n",
    "    print(\"Final_score\", final_score)\n",
    "    print(\"Best_score\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4859055,
     "sourceId": 8201984,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
